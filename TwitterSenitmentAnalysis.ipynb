{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6afd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries.\n",
    "\n",
    "import my_utils\n",
    "\n",
    "import html\n",
    "import time\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86159d2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sawan Kumar\\AppData\\Local\\Temp\\ipykernel_5824\\304028001.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  tw=pd.read_csv('Twitter Sentiments.csv', sep='\\t', header=0, error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset.\n",
    "\n",
    "#df=pd.read_csv('data.tsv', sep='\\t', header=0, error_bad_lines=False)\n",
    "tw=pd.read_csv('Twitter Sentiments.csv', sep='\\t', header=0, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05989604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id,label,tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,0, @user when a father is dysfunctional and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2,0,@user @user thanks for #lyft credit i can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3,0,  bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4,0,#model   i love u take with u all the time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5,0, factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958,0,ate @user isz that youuu?ðððð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959,0,  to see nina turner on the airwaves t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960,0,listening to sad songs on a monday mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961,1,\"@user #sikh #temple vandalised in in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962,0,thank you @user for you follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id,label,tweet\n",
       "0      1,0, @user when a father is dysfunctional and ...\n",
       "1      2,0,@user @user thanks for #lyft credit i can'...\n",
       "2                              3,0,  bihday your majesty\n",
       "3      4,0,#model   i love u take with u all the time...\n",
       "4            5,0, factsguide: society now    #motivation\n",
       "...                                                  ...\n",
       "31957  31958,0,ate @user isz that youuu?ðððð...\n",
       "31958  31959,0,  to see nina turner on the airwaves t...\n",
       "31959  31960,0,listening to sad songs on a monday mor...\n",
       "31960  31961,1,\"@user #sikh #temple vandalised in in ...\n",
       "31961           31962,0,thank you @user for you follow  \n",
       "\n",
       "[31962 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7602dd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id,label,tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,0, @user when a father is dysfunctional and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2,0,@user @user thanks for #lyft credit i can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3,0,  bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4,0,#model   i love u take with u all the time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5,0, factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6,0,[2/2] huge fan fare and big talking before...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7,0, @user camping tomorrow @user @user @user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8,0,the next school year is the year for exams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9,0,we won!!! love the land!!! #allin #cavs #c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10,0, @user @user welcome here !  i'm   it's s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id,label,tweet\n",
       "0  1,0, @user when a father is dysfunctional and ...\n",
       "1  2,0,@user @user thanks for #lyft credit i can'...\n",
       "2                          3,0,  bihday your majesty\n",
       "3  4,0,#model   i love u take with u all the time...\n",
       "4        5,0, factsguide: society now    #motivation\n",
       "5  6,0,[2/2] huge fan fare and big talking before...\n",
       "6  7,0, @user camping tomorrow @user @user @user ...\n",
       "7  8,0,the next school year is the year for exams...\n",
       "8  9,0,we won!!! love the land!!! #allin #cavs #c...\n",
       "9  10,0, @user @user welcome here !  i'm   it's s..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=tw.head(10)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c924d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We require only the review_body, star_rating columns which describes the reviews, star rating of each review respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2b20c11",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['tweet'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tw\u001b[38;5;241m=\u001b[39m\u001b[43mtw\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3810\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3809\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3810\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3812\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6111\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6109\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6111\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6113\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6115\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6171\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6170\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6173\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6174\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['tweet'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "tw=tw[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb681f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the NULL, missing values and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad609b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766743</th>\n",
       "      <td>It is nice looking and everything (it is sterl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766744</th>\n",
       "      <td>my boyfriend bought me this last christmas, an...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766745</th>\n",
       "      <td>This is a great way to quickly start learning ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766746</th>\n",
       "      <td>the 14kt gold earrings look remarkable...would...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766747</th>\n",
       "      <td>It will be a gift to my special friend. We kno...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1766748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body star_rating\n",
       "0        so beautiful even tho clearly not high end ......           5\n",
       "1        Great product.. I got this set for my mother, ...           5\n",
       "2        Exactly as pictured and my daughter's friend l...           5\n",
       "3        Love it. Fits great. Super comfortable and nea...           5\n",
       "4        Got this as a Mother's Day gift for my Mom and...           5\n",
       "...                                                    ...         ...\n",
       "1766743  It is nice looking and everything (it is sterl...           4\n",
       "1766744  my boyfriend bought me this last christmas, an...           4\n",
       "1766745  This is a great way to quickly start learning ...           4\n",
       "1766746  the 14kt gold earrings look remarkable...would...           5\n",
       "1766747  It will be a gift to my special friend. We kno...           5\n",
       "\n",
       "[1766748 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90861741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labelling Reviews: \n",
    "#The reviews with star rating 4,5 are labelled as positive reviews and \n",
    "#1,2,3 are labelled as negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295a8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling and adding the new column to the dataset.\n",
    "\n",
    "df['star_rating']=df['star_rating'].astype(int) #convert the star_rating column to int\n",
    "df['pos_neg']=np.where(df['star_rating']>=4,'pos','neg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f6e566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>pos_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I Love It Make Me Won't To Get Another</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True to size, unique</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This necklace is BEAUTIFUL......is a great acc...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>just perfect</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Love it !</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating pos_neg\n",
       "0  so beautiful even tho clearly not high end ......            5     pos\n",
       "1  Great product.. I got this set for my mother, ...            5     pos\n",
       "2  Exactly as pictured and my daughter's friend l...            5     pos\n",
       "3  Love it. Fits great. Super comfortable and nea...            5     pos\n",
       "4  Got this as a Mother's Day gift for my Mom and...            5     pos\n",
       "5             I Love It Make Me Won't To Get Another            5     pos\n",
       "6                               True to size, unique            5     pos\n",
       "7  This necklace is BEAUTIFUL......is a great acc...            5     pos\n",
       "8                                       just perfect            5     pos\n",
       "9                                          Love it !            5     pos"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a4cda90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_neg</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pos_neg                                        review_body\n",
       "0     pos  so beautiful even tho clearly not high end ......\n",
       "1     pos  Great product.. I got this set for my mother, ...\n",
       "2     pos  Exactly as pictured and my daughter's friend l...\n",
       "3     pos  Love it. Fits great. Super comfortable and nea...\n",
       "4     pos  Got this as a Mother's Day gift for my Mom and..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now considering only 2 columns\n",
    "df=df[['pos_neg','review_body']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6de58da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos_neg        0\n",
       "review_body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bd9a0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766748, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2fbdbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    1351295\n",
       "neg     415453\n",
       "Name: pos_neg, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pos_neg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01566f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample positive reveiws to get a balanced dataset\n",
    "neg = df.loc[df.pos_neg=='neg']\n",
    "pos = df.loc[df.pos_neg=='pos'].sample(n=df.pos_neg.value_counts()['neg'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf4aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "pos: 415453 , neg: 415453\n"
     ]
    }
   ],
   "source": [
    "print(type(pos))\n",
    "print(\"pos:\", len(pos), \", neg:\", len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "691afe14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_neg</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766743</th>\n",
       "      <td>pos</td>\n",
       "      <td>It is nice looking and everything (it is sterl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766744</th>\n",
       "      <td>pos</td>\n",
       "      <td>my boyfriend bought me this last christmas, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766745</th>\n",
       "      <td>pos</td>\n",
       "      <td>This is a great way to quickly start learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766746</th>\n",
       "      <td>pos</td>\n",
       "      <td>the 14kt gold earrings look remarkable...would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766747</th>\n",
       "      <td>pos</td>\n",
       "      <td>It will be a gift to my special friend. We kno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1766748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos_neg                                        review_body\n",
       "0           pos  so beautiful even tho clearly not high end ......\n",
       "1           pos  Great product.. I got this set for my mother, ...\n",
       "2           pos  Exactly as pictured and my daughter's friend l...\n",
       "3           pos  Love it. Fits great. Super comfortable and nea...\n",
       "4           pos  Got this as a Mother's Day gift for my Mom and...\n",
       "...         ...                                                ...\n",
       "1766743     pos  It is nice looking and everything (it is sterl...\n",
       "1766744     pos  my boyfriend bought me this last christmas, an...\n",
       "1766745     pos  This is a great way to quickly start learning ...\n",
       "1766746     pos  the 14kt gold earrings look remarkable...would...\n",
       "1766747     pos  It will be a gift to my special friend. We kno...\n",
       "\n",
       "[1766748 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eecd3c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38ce9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessing function \n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stopwords = sw.words('english')\n",
    "stopwords = stopwords + ['not_' + w for w in stopwords]\n",
    "\n",
    "# transform punctuation to blanks\n",
    "trans_punct = str.maketrans(string.punctuation,' '*len(string.punctuation)) \n",
    "\n",
    "# pad punctuation with blanks\n",
    "pad_punct = str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}) \n",
    "# remove \"_\" from string.punctuation\n",
    "invalidChars = str(string.punctuation.replace(\"_\", \"\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d40b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(line, ngram=1, neg_handling=True, remove_stop=False):\n",
    "    \"\"\"\n",
    "    Preprocessing the review texts\n",
    "    @params:\n",
    "        line                       - Required: the input text (Str)\n",
    "        ngram                  - Optional: number n in the n-gram model(Int, 1, 2, or 3)\n",
    "        neg_handling       - Optional: whether to perform negation handling (Boolean)\n",
    "        remove_stop        -Optional: whether to remove the stop words (Boolean)\n",
    "    \"\"\"\n",
    "        \n",
    "    line = html.unescape(str(line))\n",
    "    line = str(line).replace(\"can't\", \"can not\")\n",
    "    line = str(line).replace(\"n't\", \" not\")\n",
    "    \n",
    "    if neg_handling:\n",
    "        line = str(line).translate(pad_punct)  # If performing negation handling, pad punctuations with blanks\n",
    "        line = nltk.word_tokenize(line.lower()) # Word normalization and tokenization\n",
    "        tokens = []\n",
    "        negated = False\n",
    "        for t in line:\n",
    "            if t in ['not', 'no']:\n",
    "                negated = not negated\n",
    "            elif t in string.punctuation or not t.isalpha():\n",
    "                negated = False\n",
    "            else:\n",
    "                tokens.append('not_' + t if negated else t)  # add \"not_\" prefix to words behind \"not\", or \"no\"     \n",
    "    else:\n",
    "        line = str(line).translate(trans_punct)  # If not performing negation handling, remove punctuations\n",
    "        line = nltk.word_tokenize(line.lower()) # Word normalization and tokenization\n",
    "        tokens = line\n",
    "    \n",
    "    if ngram==2:\n",
    "        bi_tokens = list(nltk.bigrams(line))\n",
    "        bi_tokens = list(map('_'.join, bi_tokens))\n",
    "        bi_tokens = [i for i in bi_tokens if all(j not in invalidChars for j in i)]\n",
    "        tokens = tokens + bi_tokens\n",
    "\n",
    "    if ngram==3:\n",
    "        bi_tokens = list(nltk.bigrams(line))\n",
    "        bi_tokens = list(map('_'.join, bi_tokens))\n",
    "        bi_tokens = [i for i in bi_tokens if all(j not in invalidChars for j in i)]\n",
    "        tri_tokens = list(nltk.trigrams(line))\n",
    "        tri_tokens = list(map('_'.join, tri_tokens))\n",
    "        tri_tokens = [i for i in tri_tokens if all(j not in invalidChars for j in i)]\n",
    "        tokens = tokens + bi_tokens + tri_tokens    \n",
    "     \n",
    "    if remove_stop:\n",
    "        line = [lemmatizer.lemmatize(t) for t in tokens if t not in stopwords]\n",
    "    else:\n",
    "        line = [lemmatizer.lemmatize(t) for t in tokens] \n",
    "    \n",
    "    return ' '.join(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce894218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not think this book ha any decent information it is full of typo and factual error that i can not ignore'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "line = \"I don't think this book has any decent information!!! It is full of typos and factual errors that I can't ignore.\"\n",
    "\n",
    "preprocessing(line, ngram=1, neg_handling=False, remove_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9df805c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'think book decent information full typo factual error ignore'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(line, ngram=1, neg_handling=False, remove_stop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de88b68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not_think not_this not_book not_has not_any not_decent not_information it is full of typo and factual error that i can not_ignore'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(line, ngram=1, neg_handling=True, remove_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a6c9019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not_think not_this not_book not_has not_any not_decent not_information it is full of typo and factual error that i can not_ignore i_do do_not not_think think_this this_book book_has has_any any_decent decent_information it_is is_full full_of of_typos typos_and and_factual factual_errors errors_that that_i i_can can_not not_ignore i_do_not do_not_think not_think_this think_this_book this_book_has book_has_any has_any_decent any_decent_information it_is_full is_full_of full_of_typos of_typos_and typos_and_factual and_factual_errors factual_errors_that errors_that_i that_i_can i_can_not can_not_ignore'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(line, ngram=3, neg_handling=True, remove_stop=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f18fbe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pos data:  |==================================================| 100% \n"
     ]
    }
   ],
   "source": [
    "#  preprocessing the data\n",
    "# Perform data preprocessing with negation handling, tri-gram modeling, without removing the stop words\n",
    "\n",
    "# preprocessing the positive reviews\n",
    "\n",
    "pos_data = []\n",
    "n_pos = len(pos)\n",
    "for i, p in enumerate(pos['review_body']):\n",
    "    pos_data.append(preprocessing(p, ngram=3))\n",
    "    my_utils.print_progress(bar_length=50, decimals=0, iteration=i+1, total=n_pos, prefix='Preprocessing pos data: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac1dc47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing neg data:  |==================================================| 100% \n"
     ]
    }
   ],
   "source": [
    "# preprocessing the negative reviews\n",
    "\n",
    "neg_data = []\n",
    "n_neg = len(neg)\n",
    "for i, n in enumerate(neg['review_body']):\n",
    "    neg_data.append(preprocessing(n, ngram=3))\n",
    "    my_utils.print_progress(bar_length=50, decimals=0, iteration=i+1, total=n_neg, prefix='Preprocessing neg data: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the preprocessed data\n",
    "data = pos_data + neg_data\n",
    "labels = np.concatenate((pos['pos_neg'].values, neg['pos_neg'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 2.3 Split to training, validation, and test sets\n",
    "# split the dataset to training, validation, test sets by 60-20-20\n",
    "train_data, rest_data, train_labels, rest_labels = train_test_split(data, labels, test_size=0.4, \n",
    "                                                                    stratify=labels, random_state=1234)\n",
    "valid_data, test_data, valid_labels, test_labels = train_test_split(rest_data, rest_labels, test_size=0.5, \n",
    "                                                                    stratify=rest_labels, random_state=1234)\n",
    "print(\"training size = \", len(train_data), \"validation size = \", len(valid_data), \"testing size = \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc7b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Extraction\n",
    "\n",
    "# 3.1 Compute the frequency of words\n",
    "\n",
    "# Push all tokens and compute the frequency of words\n",
    "tokens = [word for line in train_data for word in nltk.word_tokenize(line)]\n",
    "word_features = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cabce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 10 most common words\n",
    "word_features.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86126cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features (words) which occur only once (This is to be used in the basic modeling process)\n",
    "topwords = [fpair[0] for fpair in list(word_features.most_common(len(word_features))) if fpair[1]>=2] \n",
    "len(topwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1dec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Vectorizer and Tf-idf term weighting\n",
    "# Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "# Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "tf_vec = TfidfVectorizer()\n",
    "\n",
    "tf_vec.fit_transform([' '.join(topwords)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Feature Extraction\n",
    "\n",
    "# Extract features from training set\n",
    "# Vocabulary is from topwords\n",
    "train_features = tf_vec.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9558140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array[n_train_data * n_features]\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad791c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from test set\n",
    "test_features = tf_vec.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf324181",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb13afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Multinomial Naive Bayes Classification Model (MultinomialNB)\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "mnb_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56cdc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred = mnb_model.predict(test_features)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92926334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "accuracy = metrics.accuracy_score(test_labels, pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de312cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(metrics.classification_report(y_true=test_labels, y_pred=pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Logistic Regression Mode (Logistic Regression)\n",
    "lgr_model = LogisticRegression()\n",
    "print(lgr_model, end='\\n'*2)\n",
    "\n",
    "\n",
    "lgr_model.fit(train_features, train_labels)\n",
    "lgr_pred = lgr_model.predict(test_features)\n",
    "\n",
    "print('Accuracy = %.5f' % metrics.accuracy_score(test_labels, lgr_pred))\n",
    "print(metrics.classification_report(y_pred=lgr_pred, y_true=test_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94303d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
