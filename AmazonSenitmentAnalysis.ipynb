{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6afd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries.\n",
    "\n",
    "import my_utils\n",
    "\n",
    "import html\n",
    "import time\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86159d2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sawan Kumar\\AppData\\Local\\Temp\\ipykernel_4348\\2527759653.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv('data.tsv', sep='\\t', header=0, error_bad_lines=False)\n",
      "Skipping line 10437: expected 15 fields, saw 22\n",
      "Skipping line 10443: expected 15 fields, saw 22\n",
      "Skipping line 19872: expected 15 fields, saw 22\n",
      "Skipping line 20055: expected 15 fields, saw 22\n",
      "Skipping line 20107: expected 15 fields, saw 22\n",
      "Skipping line 20167: expected 15 fields, saw 22\n",
      "Skipping line 53858: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 72173: expected 15 fields, saw 22\n",
      "Skipping line 84308: expected 15 fields, saw 22\n",
      "Skipping line 92156: expected 15 fields, saw 22\n",
      "Skipping line 97791: expected 15 fields, saw 22\n",
      "Skipping line 106812: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 138899: expected 15 fields, saw 22\n",
      "Skipping line 145840: expected 15 fields, saw 22\n",
      "Skipping line 192385: expected 15 fields, saw 22\n",
      "Skipping line 194126: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 222872: expected 15 fields, saw 22\n",
      "Skipping line 236587: expected 15 fields, saw 22\n",
      "Skipping line 245017: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 273895: expected 15 fields, saw 22\n",
      "Skipping line 302200: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 390306: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 395443: expected 15 fields, saw 22\n",
      "Skipping line 399564: expected 15 fields, saw 22\n",
      "Skipping line 435836: expected 15 fields, saw 22\n",
      "Skipping line 436589: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 477174: expected 15 fields, saw 22\n",
      "Skipping line 502547: expected 15 fields, saw 22\n",
      "Skipping line 514925: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 533989: expected 15 fields, saw 22\n",
      "Skipping line 534712: expected 15 fields, saw 22\n",
      "Skipping line 545993: expected 15 fields, saw 22\n",
      "Skipping line 574954: expected 15 fields, saw 22\n",
      "Skipping line 576486: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 604707: expected 15 fields, saw 22\n",
      "Skipping line 613292: expected 15 fields, saw 22\n",
      "Skipping line 622491: expected 15 fields, saw 22\n",
      "Skipping line 648193: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 686930: expected 15 fields, saw 22\n",
      "Skipping line 691337: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 728126: expected 15 fields, saw 22\n",
      "Skipping line 728320: expected 15 fields, saw 22\n",
      "Skipping line 746281: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 831631: expected 15 fields, saw 22\n",
      "Skipping line 849295: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 888781: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 962366: expected 15 fields, saw 22\n",
      "Skipping line 976631: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 983531: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1070597: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1191390: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1258986: expected 15 fields, saw 22\n",
      "Skipping line 1260478: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1318010: expected 15 fields, saw 22\n",
      "Skipping line 1375879: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1570444: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1690885: expected 15 fields, saw 22\n",
      "Skipping line 1699783: expected 15 fields, saw 22\n",
      "\n",
      "Skipping line 1756053: expected 15 fields, saw 22\n",
      "\n",
      "C:\\Users\\Sawan Kumar\\AppData\\Local\\Temp\\ipykernel_4348\\2527759653.py:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv('data.tsv', sep='\\t', header=0, error_bad_lines=False)\n",
      "C:\\Users\\Sawan Kumar\\AppData\\Local\\Temp\\ipykernel_4348\\2527759653.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  tw=pd.read_csv('Twitter Sentiments.csv', sep='\\t', header=0, error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "# reading the dataset.\n",
    "\n",
    "df=pd.read_csv('data.tsv', sep='\\t', header=0, error_bad_lines=False)\n",
    "tw=pd.read_csv('Twitter Sentiments.csv', sep='\\t', header=0, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05989604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id,label,tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,0, @user when a father is dysfunctional and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2,0,@user @user thanks for #lyft credit i can'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3,0,  bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4,0,#model   i love u take with u all the time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5,0, factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958,0,ate @user isz that youuu?ðððð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959,0,  to see nina turner on the airwaves t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960,0,listening to sad songs on a monday mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961,1,\"@user #sikh #temple vandalised in in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962,0,thank you @user for you follow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31962 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id,label,tweet\n",
       "0      1,0, @user when a father is dysfunctional and ...\n",
       "1      2,0,@user @user thanks for #lyft credit i can'...\n",
       "2                              3,0,  bihday your majesty\n",
       "3      4,0,#model   i love u take with u all the time...\n",
       "4            5,0, factsguide: society now    #motivation\n",
       "...                                                  ...\n",
       "31957  31958,0,ate @user isz that youuu?ðððð...\n",
       "31958  31959,0,  to see nina turner on the airwaves t...\n",
       "31959  31960,0,listening to sad songs on a monday mor...\n",
       "31960  31961,1,\"@user #sikh #temple vandalised in in ...\n",
       "31961           31962,0,thank you @user for you follow  \n",
       "\n",
       "[31962 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7602dd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>50423057</td>\n",
       "      <td>R135Q3VZ4DQN5N</td>\n",
       "      <td>B00JWXFDMG</td>\n",
       "      <td>657335467</td>\n",
       "      <td>Everbling Purple and Clear Briolette Drop Swar...</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Beauties!</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>11262325</td>\n",
       "      <td>R2N0QQ6R4T7YRY</td>\n",
       "      <td>B00W5T1H9W</td>\n",
       "      <td>26030170</td>\n",
       "      <td>925 Sterling Silver Finish 6ct Simulated Diamo...</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>27541121</td>\n",
       "      <td>R3N5JE5Y4T6W5M</td>\n",
       "      <td>B00M2L6KFY</td>\n",
       "      <td>697845240</td>\n",
       "      <td>Sterling Silver Circle \"Friends Forever\" Infin...</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     50423057  R135Q3VZ4DQN5N  B00JWXFDMG       657335467   \n",
       "1          US     11262325  R2N0QQ6R4T7YRY  B00W5T1H9W        26030170   \n",
       "2          US     27541121  R3N5JE5Y4T6W5M  B00M2L6KFY       697845240   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  Everbling Purple and Clear Briolette Drop Swar...          Jewelry   \n",
       "1  925 Sterling Silver Finish 6ct Simulated Diamo...          Jewelry   \n",
       "2  Sterling Silver Circle \"Friends Forever\" Infin...          Jewelry   \n",
       "\n",
       "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0           5            0.0          0.0    N                 Y   \n",
       "1           5            0.0          0.0    N                 N   \n",
       "2           5            0.0          0.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                          Beauties!   \n",
       "1                                     Great product.   \n",
       "2  Exactly as pictured and my daughter's friend l...   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  so beautiful even tho clearly not high end ......  2015-08-31  \n",
       "1  Great product.. I got this set for my mother, ...  2015-08-31  \n",
       "2  Exactly as pictured and my daughter's friend l...  2015-08-31  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=df.head(3)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c924d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We require only the review_body, star_rating columns which describes the reviews, star rating of each review respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b20c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['review_body','star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb681f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the NULL, missing values and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad609b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766743</th>\n",
       "      <td>It is nice looking and everything (it is sterl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766744</th>\n",
       "      <td>my boyfriend bought me this last christmas, an...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766745</th>\n",
       "      <td>This is a great way to quickly start learning ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766746</th>\n",
       "      <td>the 14kt gold earrings look remarkable...would...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766747</th>\n",
       "      <td>It will be a gift to my special friend. We kno...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1766748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body star_rating\n",
       "0        so beautiful even tho clearly not high end ......           5\n",
       "1        Great product.. I got this set for my mother, ...           5\n",
       "2        Exactly as pictured and my daughter's friend l...           5\n",
       "3        Love it. Fits great. Super comfortable and nea...           5\n",
       "4        Got this as a Mother's Day gift for my Mom and...           5\n",
       "...                                                    ...         ...\n",
       "1766743  It is nice looking and everything (it is sterl...           4\n",
       "1766744  my boyfriend bought me this last christmas, an...           4\n",
       "1766745  This is a great way to quickly start learning ...           4\n",
       "1766746  the 14kt gold earrings look remarkable...would...           5\n",
       "1766747  It will be a gift to my special friend. We kno...           5\n",
       "\n",
       "[1766748 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90861741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labelling Reviews: \n",
    "#The reviews with star rating 4,5 are labelled as positive reviews and \n",
    "#1,2,3 are labelled as negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "295a8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling and adding the new column to the dataset.\n",
    "\n",
    "df['star_rating']=df['star_rating'].astype(int) #convert the star_rating column to int\n",
    "df['pos_neg']=np.where(df['star_rating']>=4,'pos','neg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09f6e566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>pos_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I Love It Make Me Won't To Get Another</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True to size, unique</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This necklace is BEAUTIFUL......is a great acc...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>just perfect</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Love it !</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating pos_neg\n",
       "0  so beautiful even tho clearly not high end ......            5     pos\n",
       "1  Great product.. I got this set for my mother, ...            5     pos\n",
       "2  Exactly as pictured and my daughter's friend l...            5     pos\n",
       "3  Love it. Fits great. Super comfortable and nea...            5     pos\n",
       "4  Got this as a Mother's Day gift for my Mom and...            5     pos\n",
       "5             I Love It Make Me Won't To Get Another            5     pos\n",
       "6                               True to size, unique            5     pos\n",
       "7  This necklace is BEAUTIFUL......is a great acc...            5     pos\n",
       "8                                       just perfect            5     pos\n",
       "9                                          Love it !            5     pos"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a4cda90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_neg</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pos_neg                                        review_body\n",
       "0     pos  so beautiful even tho clearly not high end ......\n",
       "1     pos  Great product.. I got this set for my mother, ...\n",
       "2     pos  Exactly as pictured and my daughter's friend l...\n",
       "3     pos  Love it. Fits great. Super comfortable and nea...\n",
       "4     pos  Got this as a Mother's Day gift for my Mom and..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now considering only 2 columns\n",
    "df=df[['pos_neg','review_body']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6de58da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos_neg        0\n",
       "review_body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bd9a0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1766748, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2fbdbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    1351295\n",
       "neg     415453\n",
       "Name: pos_neg, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pos_neg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01566f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample positive reveiws to get a balanced dataset\n",
    "neg = df.loc[df.pos_neg=='neg']\n",
    "pos = df.loc[df.pos_neg=='pos'].sample(n=df.pos_neg.value_counts()['neg'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf4aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "pos: 415453 , neg: 415453\n"
     ]
    }
   ],
   "source": [
    "print(type(pos))\n",
    "print(\"pos:\", len(pos), \", neg:\", len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "691afe14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_neg</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>so beautiful even tho clearly not high end ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>Great product.. I got this set for my mother, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Exactly as pictured and my daughter's friend l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Love it. Fits great. Super comfortable and nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Got this as a Mother's Day gift for my Mom and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766743</th>\n",
       "      <td>pos</td>\n",
       "      <td>It is nice looking and everything (it is sterl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766744</th>\n",
       "      <td>pos</td>\n",
       "      <td>my boyfriend bought me this last christmas, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766745</th>\n",
       "      <td>pos</td>\n",
       "      <td>This is a great way to quickly start learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766746</th>\n",
       "      <td>pos</td>\n",
       "      <td>the 14kt gold earrings look remarkable...would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766747</th>\n",
       "      <td>pos</td>\n",
       "      <td>It will be a gift to my special friend. We kno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1766748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos_neg                                        review_body\n",
       "0           pos  so beautiful even tho clearly not high end ......\n",
       "1           pos  Great product.. I got this set for my mother, ...\n",
       "2           pos  Exactly as pictured and my daughter's friend l...\n",
       "3           pos  Love it. Fits great. Super comfortable and nea...\n",
       "4           pos  Got this as a Mother's Day gift for my Mom and...\n",
       "...         ...                                                ...\n",
       "1766743     pos  It is nice looking and everything (it is sterl...\n",
       "1766744     pos  my boyfriend bought me this last christmas, an...\n",
       "1766745     pos  This is a great way to quickly start learning ...\n",
       "1766746     pos  the 14kt gold earrings look remarkable...would...\n",
       "1766747     pos  It will be a gift to my special friend. We kno...\n",
       "\n",
       "[1766748 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79e29bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package nombank.1.0 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Sawan Kumar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package vader_lexicon to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\Sawan\n",
      "[nltk_data]    |     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38ce9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessing function \n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stopwords = sw.words('english')\n",
    "stopwords = stopwords + ['not_' + w for w in stopwords]\n",
    "\n",
    "# transform punctuation to blanks\n",
    "trans_punct = str.maketrans(string.punctuation,' '*len(string.punctuation)) \n",
    "\n",
    "# pad punctuation with blanks\n",
    "pad_punct = str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}) \n",
    "# remove \"_\" from string.punctuation\n",
    "invalidChars = str(string.punctuation.replace(\"_\", \"\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d40b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(line, ngram=1, neg_handling=True, remove_stop=False):\n",
    "    \"\"\"\n",
    "    Preprocessing the review texts\n",
    "    @params:\n",
    "        line                       - Required: the input text (Str)\n",
    "        ngram                  - Optional: number n in the n-gram model(Int, 1, 2, or 3)\n",
    "        neg_handling       - Optional: whether to perform negation handling (Boolean)\n",
    "        remove_stop        -Optional: whether to remove the stop words (Boolean)\n",
    "    \"\"\"\n",
    "        \n",
    "    line = html.unescape(str(line))\n",
    "    line = str(line).replace(\"can't\", \"can not\")\n",
    "    line = str(line).replace(\"n't\", \" not\")\n",
    "    \n",
    "    if neg_handling:\n",
    "        line = str(line).translate(pad_punct)  # If performing negation handling, pad punctuations with blanks\n",
    "        line = nltk.word_tokenize(line.lower()) # Word normalization and tokenization\n",
    "        tokens = []\n",
    "        negated = False\n",
    "        for t in line:\n",
    "            if t in ['not', 'no']:\n",
    "                negated = not negated\n",
    "            elif t in string.punctuation or not t.isalpha():\n",
    "                negated = False\n",
    "            else:\n",
    "                tokens.append('not_' + t if negated else t)  # add \"not_\" prefix to words behind \"not\", or \"no\"     \n",
    "    else:\n",
    "        line = str(line).translate(trans_punct)  # If not performing negation handling, remove punctuations\n",
    "        line = nltk.word_tokenize(line.lower()) # Word normalization and tokenization\n",
    "        tokens = line\n",
    "    \n",
    "    if ngram==2:\n",
    "        bi_tokens = list(nltk.bigrams(line))\n",
    "        bi_tokens = list(map('_'.join, bi_tokens))\n",
    "        bi_tokens = [i for i in bi_tokens if all(j not in invalidChars for j in i)]\n",
    "        tokens = tokens + bi_tokens\n",
    "\n",
    "    if ngram==3:\n",
    "        bi_tokens = list(nltk.bigrams(line))\n",
    "        bi_tokens = list(map('_'.join, bi_tokens))\n",
    "        bi_tokens = [i for i in bi_tokens if all(j not in invalidChars for j in i)]\n",
    "        tri_tokens = list(nltk.trigrams(line))\n",
    "        tri_tokens = list(map('_'.join, tri_tokens))\n",
    "        tri_tokens = [i for i in tri_tokens if all(j not in invalidChars for j in i)]\n",
    "        tokens = tokens + bi_tokens + tri_tokens    \n",
    "     \n",
    "    if remove_stop:\n",
    "        line = [lemmatizer.lemmatize(t) for t in tokens if t not in stopwords]\n",
    "    else:\n",
    "        line = [lemmatizer.lemmatize(t) for t in tokens] \n",
    "    \n",
    "    return ' '.join(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce894218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not think this book ha any decent information it is full of typo and factual error that i can not ignore'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "line = \"I don't think this book has any decent information!!! It is full of typos and factual errors that I can't ignore.\"\n",
    "\n",
    "preprocessing(line, ngram=1, neg_handling=False, remove_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9df805c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'think book decent information full typo factual error ignore'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(line, ngram=1, neg_handling=False, remove_stop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de88b68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not_think not_this not_book not_has not_any not_decent not_information it is full of typo and factual error that i can not_ignore'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(line, ngram=1, neg_handling=True, remove_stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a6c9019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i do not_think not_this not_book not_has not_any not_decent not_information it is full of typo and factual error that i can not_ignore i_do do_not not_think think_this this_book book_has has_any any_decent decent_information it_is is_full full_of of_typos typos_and and_factual factual_errors errors_that that_i i_can can_not not_ignore i_do_not do_not_think not_think_this think_this_book this_book_has book_has_any has_any_decent any_decent_information it_is_full is_full_of full_of_typos of_typos_and typos_and_factual and_factual_errors factual_errors_that errors_that_i that_i_can i_can_not can_not_ignore'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(line, ngram=3, neg_handling=True, remove_stop=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f18fbe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pos data:  |==================================================| 100% \n"
     ]
    }
   ],
   "source": [
    "#  preprocessing the data\n",
    "# Perform data preprocessing with negation handling, tri-gram modeling, without removing the stop words\n",
    "\n",
    "# preprocessing the positive reviews\n",
    "\n",
    "pos_data = []\n",
    "n_pos = len(pos)\n",
    "for i, p in enumerate(pos['review_body']):\n",
    "    pos_data.append(preprocessing(p, ngram=3))\n",
    "    my_utils.print_progress(bar_length=50, decimals=0, iteration=i+1, total=n_pos, prefix='Preprocessing pos data: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac1dc47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing neg data:  |==================================================| 100% \n"
     ]
    }
   ],
   "source": [
    "# preprocessing the negative reviews\n",
    "\n",
    "neg_data = []\n",
    "n_neg = len(neg)\n",
    "for i, n in enumerate(neg['review_body']):\n",
    "    neg_data.append(preprocessing(n, ngram=3))\n",
    "    my_utils.print_progress(bar_length=50, decimals=0, iteration=i+1, total=n_neg, prefix='Preprocessing neg data: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b4e8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the preprocessed data\n",
    "data = pos_data + neg_data\n",
    "labels = np.concatenate((pos['pos_neg'].values, neg['pos_neg'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9abb16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size =  498543 validation size =  166181 testing size =  166182\n"
     ]
    }
   ],
   "source": [
    " # 2.3 Split to training, validation, and test sets\n",
    "# split the dataset to training, validation, test sets by 60-20-20\n",
    "train_data, rest_data, train_labels, rest_labels = train_test_split(data, labels, test_size=0.4, \n",
    "                                                                    stratify=labels, random_state=1234)\n",
    "valid_data, test_data, valid_labels, test_labels = train_test_split(rest_data, rest_labels, test_size=0.5, \n",
    "                                                                    stratify=rest_labels, random_state=1234)\n",
    "print(\"training size = \", len(train_data), \"validation size = \", len(valid_data), \"testing size = \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bc7b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Extraction\n",
    "\n",
    "# 3.1 Compute the frequency of words\n",
    "\n",
    "# Push all tokens and compute the frequency of words\n",
    "tokens = [word for line in train_data for word in nltk.word_tokenize(line)]\n",
    "word_features = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5cabce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 4431451 samples and 45321257 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "881b3f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 740895),\n",
       " ('i', 707771),\n",
       " ('it', 627125),\n",
       " ('a', 506860),\n",
       " ('and', 459898),\n",
       " ('to', 299497),\n",
       " ('is', 297073),\n",
       " ('for', 235938),\n",
       " ('this', 235047),\n",
       " ('wa', 217261)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the 10 most common words\n",
    "word_features.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86126cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1329061"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove features (words) which occur only once (This is to be used in the basic modeling process)\n",
    "topwords = [fpair[0] for fpair in list(word_features.most_common(len(word_features))) if fpair[1]>=2] \n",
    "len(topwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c1dec58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1328813 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1328813 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.2 Vectorizer and Tf-idf term weighting\n",
    "# Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "# Equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "tf_vec = TfidfVectorizer()\n",
    "\n",
    "tf_vec.fit_transform([' '.join(topwords)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a44f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Feature Extraction\n",
    "\n",
    "# Extract features from training set\n",
    "# Vocabulary is from topwords\n",
    "train_features = tf_vec.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9558140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498543, 1328813)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Array[n_train_data * n_features]\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad791c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from test set\n",
    "test_features = tf_vec.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf324181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166182, 1328813)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5eb13afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.1 Multinomial Naive Bayes Classification Model (MultinomialNB)\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "332bdbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "mnb_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b56cdc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'neg' 'pos' ... 'neg' 'neg' 'pos']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "pred = mnb_model.predict(test_features)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92926334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9008797583372448\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "accuracy = metrics.accuracy_score(test_labels, pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de312cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg     0.8850    0.9215    0.9029     83091\n",
      "         pos     0.9182    0.8802    0.8988     83091\n",
      "\n",
      "    accuracy                         0.9009    166182\n",
      "   macro avg     0.9016    0.9009    0.9008    166182\n",
      "weighted avg     0.9016    0.9009    0.9008    166182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(metrics.classification_report(y_true=test_labels, y_pred=pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4af9243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sawan Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.91252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg     0.9066    0.9198    0.9131     83091\n",
      "         pos     0.9186    0.9052    0.9119     83091\n",
      "\n",
      "    accuracy                         0.9125    166182\n",
      "   macro avg     0.9126    0.9125    0.9125    166182\n",
      "weighted avg     0.9126    0.9125    0.9125    166182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.2 Logistic Regression Mode (Logistic Regression)\n",
    "lgr_model = LogisticRegression()\n",
    "print(lgr_model, end='\\n'*2)\n",
    "\n",
    "\n",
    "lgr_model.fit(train_features, train_labels)\n",
    "lgr_pred = lgr_model.predict(test_features)\n",
    "\n",
    "print('Accuracy = %.5f' % metrics.accuracy_score(test_labels, lgr_pred))\n",
    "print(metrics.classification_report(y_pred=lgr_pred, y_true=test_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b3598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
